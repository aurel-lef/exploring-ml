{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble classifier on Titanic Kaggle dataset with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives: full pyspark notebook\n",
    "\n",
    "The primary goal of this notebook is to make a deep dive into spark ML module:  \n",
    "- using the well known kaggle Titanic dataset (already explored in a sklearn notebook)  \n",
    "- performing feature extraction and preprocessing using custom Transformers/Estimators pipelines  \n",
    "- combining trained models into a custom majority vote Ensemble  \n",
    "- preparing the ground for intensive hyperparameter tuning on a google gcp hadoop cluster  \n",
    "\n",
    "<br>\n",
    "\n",
    "## Note  \n",
    "I opted for custom Estimators and Models especially for:  \n",
    "- the imputation of missing _\"Age\"_ values via linear regression of features _\"Pclass\"_ and _\"Sex\"_  \n",
    "- the creation of a majority Vote Ensemble classifier   \n",
    "\n",
    "**Feedbacks are obviously welcome!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\n",
      "1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S\n",
      "2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C\n"
     ]
    }
   ],
   "source": [
    "# Let's vizualize the csv dataset format\n",
    "!powershell Get-Content \"../data/train.csv\" -Head 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset - quick viz\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "train dataset - nb missing values per column\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n",
      "test dataset - nb missing values per column\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|     0|   0|  0| 86|    0|    0|     0|   1|  327|       0|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, Column\n",
    "from pyspark.sql.functions import count, when, col\n",
    "\n",
    "spark = ( SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"votingclassifier-titanic\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "train_df = spark.read.csv(\n",
    "    \"../data/train.csv\", \n",
    "    header = True,\n",
    "    inferSchema = True\n",
    ").cache()\n",
    "\n",
    "test_df = spark.read.csv(\n",
    "    \"../data/test.csv\", \n",
    "    header = True,\n",
    "    inferSchema = True\n",
    ").cache()\n",
    "\n",
    "def display_missing_values(df):\n",
    "        df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "print(\"train dataset - quick viz\")\n",
    "train_df.printSchema()\n",
    "train_df.show(5)\n",
    "\n",
    "print(\"train dataset - nb missing values per column\")\n",
    "display_missing_values(train_df)\n",
    "\n",
    "print(\"test dataset - nb missing values per column\")\n",
    "display_missing_values(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "We already performed the feature engineering work with pandas in the sklearn notebook.  \n",
    "Here we applied our findings using pyspark.  \n",
    "Since Pyspark does not provide regression Imputer, we create a custome one for \"Age\" missing values regression from \"Pclass\" and \"Sex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset - features extracted\n",
      "+-----------+--------+--------+-----------+-----+--------------+-------------+------------------+------------+\n",
      "|PassengerId|Survived|Embarked|Accompanied|Title|Pclass_encoded|  Sex_encoded|       Age_imputed|Fare_imputed|\n",
      "+-----------+--------+--------+-----------+-----+--------------+-------------+------------------+------------+\n",
      "|          6|       0|       Q|          0|  Mr.| (4,[0],[1.0])|(3,[0],[1.0])|26.497742339152797|      8.4583|\n",
      "|         18|       1|       S|          0|  Mr.| (4,[2],[1.0])|(3,[0],[1.0])| 31.81582649258425|        13.0|\n",
      "|         20|       1|       C|          0| Mrs.| (4,[0],[1.0])|(3,[1],[1.0])|21.982979570371622|       7.225|\n",
      "|         27|       0|       C|          0|  Mr.| (4,[0],[1.0])|(3,[0],[1.0])|26.497742339152797|       7.225|\n",
      "|         29|       1|       Q|          0|Miss.| (4,[0],[1.0])|(3,[1],[1.0])|21.982979570371622|      7.8792|\n",
      "+-----------+--------+--------+-----------+-----+--------------+-------------+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "train dataset - nb missing values per column\n",
      "+-----------+--------+--------+-----------+-----+--------------+-----------+-----------+------------+\n",
      "|PassengerId|Survived|Embarked|Accompanied|Title|Pclass_encoded|Sex_encoded|Age_imputed|Fare_imputed|\n",
      "+-----------+--------+--------+-----------+-----+--------------+-----------+-----------+------------+\n",
      "|          0|       0|       0|          0|    0|             0|          0|          0|           0|\n",
      "+-----------+--------+--------+-----------+-----+--------------+-----------+-----------+------------+\n",
      "\n",
      "test dataset - features extracted\n",
      "+-----------+--------+-----------+-----+--------------+-------------+------------------+------------+\n",
      "|PassengerId|Embarked|Accompanied|Title|Pclass_encoded|  Sex_encoded|       Age_imputed|Fare_imputed|\n",
      "+-----------+--------+-----------+-----+--------------+-------------+------------------+------------+\n",
      "|        902|       S|          0|  Mr.| (4,[0],[1.0])|(3,[0],[1.0])|26.497742339152797|      7.8958|\n",
      "|        914|       S|          0| Mrs.| (4,[1],[1.0])|(3,[1],[1.0])|35.660971473345526|     31.6833|\n",
      "|        921|       C|          1|  Mr.| (4,[0],[1.0])|(3,[0],[1.0])|26.497742339152797|     21.6792|\n",
      "|        925|       S|          1| Mrs.| (4,[0],[1.0])|(3,[1],[1.0])|21.982979570371622|       23.45|\n",
      "|        928|       S|          0|Miss.| (4,[0],[1.0])|(3,[1],[1.0])|21.982979570371622|        8.05|\n",
      "+-----------+--------+-----------+-----+--------------+-------------+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "test dataset - nb missing values per column\n",
      "+-----------+--------+-----------+-----+--------------+-----------+-----------+------------+\n",
      "|PassengerId|Embarked|Accompanied|Title|Pclass_encoded|Sex_encoded|Age_imputed|Fare_imputed|\n",
      "+-----------+--------+-----------+-----+--------------+-----------+-----------+------------+\n",
      "|          0|       0|          0|    0|             0|          0|          0|           0|\n",
      "+-----------+--------+-----------+-----+--------------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import regexp_extract, udf\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler, Imputer\n",
    "from pyspark.ml import Pipeline, Transformer, Estimator, Model\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "# Accompanied = binary feature = \"has either sibling, spouse, parent or child\"\n",
    "# stateless transformation => use a transformer\n",
    "class ExtractAccompaniedFeature(Transformer):\n",
    "    def transform(self, dataset, params=None):\n",
    "        return dataset.withColumn(\n",
    "            \"Accompanied\", \n",
    "            (dataset.SibSp + dataset.Parch >= 1).cast(IntegerType()) \n",
    "        )\n",
    "\n",
    "# Imput missing Embarked with most frequent value\n",
    "# stateful transformation => use an estimator\n",
    "class HandleMissingEmbarked(Estimator):\n",
    "    def fit(self, dataset, params=None):\n",
    "        mostFrequentValue = (dataset.groupby(\"Embarked\")\n",
    "                             .count()\n",
    "                             .orderBy(\"count\", ascending=False)\n",
    "                             .first()\n",
    "                             .Embarked\n",
    "                            )\n",
    "        return HandleMissingEmbarkedModel(mostFrequentValue)\n",
    "        \n",
    "class HandleMissingEmbarkedModel(Model):\n",
    "    \n",
    "    def __init__(self, mostFrequentValue):\n",
    "        self.mostFrequentValue = mostFrequentValue\n",
    "        \n",
    "    def transform(self, dataset, params=None):\n",
    "        return dataset.fillna(self.mostFrequentValue, \"Embarked\")\n",
    "\n",
    "# TItle regex processing\n",
    "@udf(returnType=StringType())\n",
    "def replace_title(s):\n",
    "    mrs_pattern = \"(Mme\\.|Ms\\.|Countess\\.|Lady\\.)\"\n",
    "    miss_pattern = \"(Mlle\\.)\"\n",
    "    mr_pattern = \"(Don\\.|Major\\.|Sir\\.|Col\\.|Capt\\.)\"\n",
    "    if re.search(mrs_pattern, s):\n",
    "        return re.sub(mrs_pattern, \"Mrs.\", s)\n",
    "    if re.search(miss_pattern, s):\n",
    "        return re.sub(miss_pattern, \"Miss.\", s)\n",
    "    if re.search(mr_pattern, s):\n",
    "        return re.sub(mr_pattern, \"Mr.\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "@udf\n",
    "def replace_empty(s):\n",
    "    if s == \"\":\n",
    "        return \"No-Title\"\n",
    "    return s\n",
    "\n",
    "# extraction of Title feature from Name\n",
    "class ExtractTitle(Transformer):\n",
    "    def transform(self, dataset, params=None):\n",
    "        titles_extract_pattern = r'(Mr\\.|Mrs\\.|Miss\\.|Master\\.|Dr\\.|Rev\\.)'\n",
    "        return ( dataset.withColumn(\"Title\", regexp_extract(\"Name\", titles_extract_pattern, 1))\n",
    "                .withColumn(\"Title\", replace_empty(\"Title\"))\n",
    "               )\n",
    "    \n",
    "# Imputing missing \"Age\" from regression of Pclass and Sex\n",
    "class HandleMissingAge(Estimator):\n",
    "    def __init__(self):\n",
    "        vect = VectorAssembler(\n",
    "            inputCols = [\"Pclass_encoded\", \"Sex_encoded\"], \n",
    "            outputCol='features_class_sex'\n",
    "        )\n",
    "        \n",
    "        lr = LinearRegression(\n",
    "            featuresCol=\"features_class_sex\",\n",
    "            labelCol='Age',\n",
    "            predictionCol='Age_imputed',\n",
    "            regParam = 0.3\n",
    "        )\n",
    "\n",
    "        self.pipe = Pipeline(\n",
    "            stages = [\n",
    "                vect,\n",
    "                lr\n",
    "            ])\n",
    "        self._params = None\n",
    "        self._paramMap = ParamGridBuilder().build()\n",
    "\n",
    "\n",
    "    def fit(self, dataset, params=None):\n",
    "        dataset_without_missing = dataset.where(col(\"Age\").isNotNull())\n",
    "        ageRegressor = self.pipe.fit(dataset_without_missing)\n",
    "        return HandleMissingAgeModel(ageRegressor)\n",
    "\n",
    "    \n",
    "class HandleMissingAgeModel(Model):\n",
    "    \n",
    "    def __init__(self, ageRegressor):\n",
    "        self.ageRegressor = ageRegressor\n",
    "        \n",
    "    def transform(self, dataset, params=None):\n",
    "        null_age_df = dataset.where(col(\"Age\").isNull())\n",
    "        not_null_age_df = dataset.where(col(\"Age\").isNotNull())\n",
    "              \n",
    "        null_age_df = (\n",
    "            self.ageRegressor\n",
    "            .transform(null_age_df)\n",
    "            .drop(\"features_class_sex\")\n",
    "            .cache()\n",
    "        )\n",
    "            \n",
    "        return null_age_df.union(\n",
    "            not_null_age_df.withColumn(\"Age_imputed\", col(\"Age\"))\n",
    "        )\n",
    "    \n",
    "pipe_extractFeatures = Pipeline(\n",
    "    stages = [\n",
    "        ExtractAccompaniedFeature(),\n",
    "        ExtractTitle(),\n",
    "        HandleMissingEmbarked(),\n",
    "        \n",
    "        # need to get Dummy categorisation of Pclass & Sex for Age Imputation\n",
    "        StringIndexer(inputCol = \"Pclass\", outputCol='Pclass_indexed', handleInvalid='keep'),\n",
    "        StringIndexer(inputCol = \"Sex\", outputCol='Sex_indexed', handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol = \"Pclass_indexed\", outputCol='Pclass_encoded', handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol = \"Sex_indexed\", outputCol='Sex_encoded', handleInvalid='keep'),\n",
    "        \n",
    "        HandleMissingAge(),\n",
    "        Imputer(inputCol = \"Fare\", outputCol='Fare_imputed')       \n",
    "])\n",
    "\n",
    "keep_features = [\"PassengerId\", \"Survived\", \"Pclass_encoded\", \"Title\", \"Sex_encoded\", \n",
    "                 \"Age_imputed\", \"Fare_imputed\", \"Embarked\", \"Accompanied\"]\n",
    "\n",
    "pipe_extractFeatures_fitted = pipe_extractFeatures.fit(train_df)\n",
    "\n",
    "def extract_features(df):\n",
    "    return (pipe_extractFeatures_fitted.transform(df)\n",
    "            .transform(lambda df: df.select([col for col in df.columns if col in keep_features])))\n",
    "\n",
    "\n",
    "train = extract_features(train_df).cache()\n",
    "test = extract_features(test_df).cache()\n",
    "train_df.unpersist()\n",
    "test_df.unpersist()\n",
    "\n",
    "print(\"train dataset - features extracted\")\n",
    "train.show(5)\n",
    "\n",
    "print(\"train dataset - nb missing values per column\")\n",
    "display_missing_values(train)\n",
    "\n",
    "print(\"test dataset - features extracted\")\n",
    "test.show(5)\n",
    "\n",
    "print(\"test dataset - nb missing values per column\")\n",
    "display_missing_values(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of RandomForest, GBT and MLP classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train these three models using a 3-Fold Cross validation.  \n",
    "Hyperparameter tuning is kept aside for now, we'll perform it later on a cloud hadoop cluster (see gcp section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlp] avg accuracy:\n",
      "0.8020948608210172\n",
      "\n",
      "[rf] avg accuracy:\n",
      "0.8196452797511349\n",
      "\n",
      "[gbt] avg accuracy:\n",
      "0.8174916361138569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "preprocess = Pipeline(\n",
    "    stages = [\n",
    "        StringIndexer(inputCol = \"Embarked\", outputCol='Embarked_indexed', handleInvalid='keep'),\n",
    "        StringIndexer(inputCol = \"Title\", outputCol='Title_indexed', handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol = \"Embarked_indexed\", outputCol='Embarked_encoded', handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol = \"Accompanied\", outputCol='Accompanied_encoded', handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol = \"Title_indexed\", outputCol='Title_encoded', handleInvalid='keep'),\n",
    "        VectorAssembler(inputCols = [\"Fare_imputed\"], outputCol='Fare_vect'),\n",
    "        StandardScaler(withMean = True, inputCol = \"Fare_vect\", outputCol='Fare_std'),\n",
    "        VectorAssembler(inputCols = [\"Age_imputed\"], outputCol='Age_vect'),\n",
    "        StandardScaler(withMean = True, inputCol = \"Age_vect\", outputCol='Age_std'),        \n",
    "])\n",
    "\n",
    "inputCols = [\n",
    "    \"Pclass_encoded\", \n",
    "    \"Sex_encoded\", \n",
    "    \"Embarked_encoded\", \n",
    "    \"Accompanied_encoded\", \n",
    "    \"Title_encoded\",\n",
    "    \"Age_std\", \n",
    "    \"Fare_std\"]\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol = \"Survived\")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol = \"Survived\")\n",
    "\n",
    "va = VectorAssembler(\n",
    "    inputCols = inputCols,\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# here we have 23 inputs features corresponding to the 7 features in inputCols which are already encoded\n",
    "# can be checked with: \n",
    "# transformed = pipe_preprocessing.fit(train_df).transform(train_df)\n",
    "# va.transform(transformed).schema[\"features\"].metadata[\"ml_attr\"]\n",
    "layers = [23, 100, 2]\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol = \"Survived\",\n",
    "    layers=layers\n",
    ")\n",
    "\n",
    "# Yes, accuracy is rather a poor evaluation metric\n",
    "# but it is the metric defined in the Titanic kaggle competition\n",
    "# the only way to get an accuracy evaluator seems to use MulticlassClassificationEvaluator\n",
    "accuracyEvaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", metricName=\"accuracy\")\n",
    "    \n",
    "\n",
    "trainedModels = dict()\n",
    "for classifier, classifier_name in zip( [mlp, rf, gbt], [\"mlp\", \"rf\", \"gbt\"]):\n",
    "    \n",
    "    clf = Pipeline(\n",
    "        stages = [\n",
    "        preprocess,\n",
    "        va,\n",
    "        classifier\n",
    "    ])\n",
    "\n",
    "    # no paramter tuning for now, will perform it later on a gcp hadoop cluster    \n",
    "    paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "    # example of parameter tuning for GBT:\n",
    "    # paramGrid = ParamGridBuilder()\n",
    "    #   .addGrid(gbt.stepSize, [0.001, 0.03, 0.1, 0.3])\n",
    "    #   .addGrid(gbt.maxDepth, list(range(3,10))\n",
    "    #   .build()\n",
    "\n",
    "    cv = CrossValidator(\n",
    "        estimator=clf, \n",
    "        estimatorParamMaps=paramGrid, \n",
    "        evaluator=accuracyEvaluator, \n",
    "        numFolds=3, \n",
    "        parallelism=2)\n",
    "\n",
    "    cv_model = cv.fit(train)\n",
    "    trainedModels[classifier_name] = cv_model.bestModel\n",
    "    print(f\"[{classifier_name}] avg accuracy:\\n{cv_model.avgMetrics[0]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(gbt.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Vote Ensemble Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement here the equivalent to sklearn.ensemble.VotingClassifier in Pyspark.  \n",
    "The goal using such an ensemble being to obtain better overall performances and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import array\n",
    "\n",
    "@udf(returnType = DoubleType())\n",
    "def majority_vote_prediction(predictions):\n",
    "    return float(np.mean(predictions) >= 0.5)\n",
    "\n",
    "@udf(returnType = DoubleType())\n",
    "def majority_vote_proba(predictions, probas):\n",
    "    prediction = int(np.mean(predictions) >= 0.5)\n",
    "    if prediction == 0:\n",
    "        # high probability for the class [Survived == 0]\n",
    "        return float(np.max(probas))\n",
    "    else:\n",
    "        return float(np.min(probas))\n",
    "    \n",
    "# Custom implementation of a majority vote Ensemble of trained ML models\n",
    "# should be equivalent of its scikit-learn counterpart\n",
    "# TODO: make it more generic to fit any ensemble of fitted models: currently adherence with Titanic dataset features names\n",
    "\n",
    "class VotingClassifier(Model, Estimator):\n",
    "    \n",
    "    def __init__(self, fittedModels):\n",
    "        self.fittedModels = fittedModels\n",
    "    \n",
    "    @staticmethod\n",
    "    def concatenate(final_df, current_df):\n",
    "        keeped = [\"PassengerId\", \"prediction\", \"probability\"]\n",
    "        if(final_df == None):\n",
    "            # keep labels \"Survived\" column\n",
    "            return current_df.select([col for col in current_df.columns if col in keeped + [\"Survived\"]])\n",
    "        return final_df.join(current_df.select(keeped), \"PassengerId\")\n",
    "        \n",
    "    def transform(self, dataset, params=None):\n",
    "        all_predictions_df = None\n",
    "        for model_name, model in self.fittedModels.items():\n",
    "            all_predictions_df = (\n",
    "                model.transform(dataset)\n",
    "                .transform(lambda df: VotingClassifier.concatenate(all_predictions_df, df))\n",
    "                .withColumnRenamed(\"prediction\", f\"{model_name}_prediction\")\n",
    "                .withColumnRenamed(\"probability\", f\"{model_name}_probability\")\n",
    "             )\n",
    "        \n",
    "        predictionCols = [col for col in all_predictions_df.columns if \"_prediction\" in col]\n",
    "        probabilityCols = [col for col in all_predictions_df.columns if \"_probability\" in col]\n",
    "        return (\n",
    "            all_predictions_df\n",
    "            .withColumn(\n",
    "                \"prediction\", \n",
    "                majority_vote_prediction(array([col(c) for c in predictionCols])))\n",
    "            .withColumn(\n",
    "                \"probability\", \n",
    "                majority_vote_proba(array([col(c) for c in predictionCols]), array([col(c) for c in probabilityCols])))\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions with the ensemble model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "votingClf = VotingClassifier(trainedModels)\n",
    "final_prediction = votingClf.transform(test).cache()\n",
    "final_prediction.show(5)\n",
    "    \n",
    "# store final prediction for kaggle submission\n",
    "(\n",
    "    final_prediction\n",
    "    .select(col(\"PassengerId\"), col(\"prediction\").cast(IntegerType()))\n",
    "    .withColumnRenamed(\"prediction\", \"Survived\")\n",
    "    .coalesce(1)\n",
    "    .orderBy(\"PassengerId\")\n",
    "    .write.csv(\n",
    "        \"../submissions/titanic_ensemble_rf_gb_mlp_spark.csv\",\n",
    "        mode = \"overwrite\",\n",
    "        header = True\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
