{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble classifier on Titanic Kaggle dataset with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created this notebook to take a deep dive into the spark ML module.  \n",
    "I re-used the Titanic dataset, which I already explored in a previous sklearn notebook.\n",
    "\n",
    "\n",
    "The main objectives of this notebook are:\n",
    "- perform a full pipeline of feature extraction, preprocessing and model training\n",
    "- combine well performing models in a majority vote Ensemble classifier\n",
    "- prepare the code before running an intensive hyperparameter tuning on a cloud hadoop cluster\n",
    "<br> \n",
    "<br>\n",
    "  \n",
    "I opted for the creation of custom Estimators, Transformers and Models to design the pipelines, especially for:\n",
    "- the imputation of missing _\"Age\"_ values via linear regression of features _\"Pclass\"_ and _\"Sex\"_\n",
    "- the creation of a majority Vote Ensemble classifier  \n",
    "<br> \n",
    "\n",
    "Note:  \n",
    "- The feature engineering peparation work is largely skipped in this notebook since it was already largely performed in the pandas/sklearn notebook on this same dataset.  \n",
    "- **Any feeback or enhancement tip is welcome!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\n",
      "1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S\n",
      "2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C\n"
     ]
    }
   ],
   "source": [
    "# Let's vizualize the csv dataset format\n",
    "!powershell Get-Content \"../data/train.csv\" -Head 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|     0|   0|  0| 86|    0|    0|     0|   1|  327|       0|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, Column\n",
    "from pyspark.sql.functions import count, when, col\n",
    "\n",
    "spark = ( SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"votingclassifier-titanic\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "train_df = spark.read.csv(\n",
    "    \"../data/train.csv\", \n",
    "    header = True,\n",
    "    inferSchema = True\n",
    ")\n",
    "\n",
    "test_df = spark.read.csv(\n",
    "    \"../data/test.csv\", \n",
    "    header = True,\n",
    "    inferSchema = True\n",
    ")\n",
    "\n",
    "def display_missing_values(df):\n",
    "        df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "train_df.printSchema()\n",
    "train_df.show(5)\n",
    "\n",
    "display_missing_values(train_df)\n",
    "display_missing_values(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+------+------+-----+--------+-----------+-----+--------------+-----------+--------------+-------------+------------------+------------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|Ticket|  Fare|Cabin|Embarked|Accompanied|Title|Pclass_indexed|Sex_indexed|Pclass_encoded|  Sex_encoded|       Age_imputed|Fare_imputed|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+------+------+-----+--------+-----------+-----+--------------+-----------+--------------+-------------+------------------+------------+\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|330877|8.4583| null|       Q|          0|  Mr.|           0.0|        0.0| (4,[0],[1.0])|(3,[0],[1.0])|26.497742339152797|      8.4583|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|244373|  13.0| null|       S|          0|  Mr.|           2.0|        0.0| (4,[2],[1.0])|(3,[0],[1.0])| 31.81582649258425|        13.0|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|  2649| 7.225| null|       C|          0| Mrs.|           0.0|        1.0| (4,[0],[1.0])|(3,[1],[1.0])|21.982979570371622|       7.225|\n",
      "|         27|       0|     3|Emir, Mr. Farred ...|  male|null|    0|    0|  2631| 7.225| null|       C|          0|  Mr.|           0.0|        0.0| (4,[0],[1.0])|(3,[0],[1.0])|26.497742339152797|       7.225|\n",
      "|         29|       1|     3|\"O'Dwyer, Miss. E...|female|null|    0|    0|330959|7.8792| null|       Q|          0|Miss.|           0.0|        1.0| (4,[0],[1.0])|(3,[1],[1.0])|21.982979570371622|      7.8792|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+------+------+-----+--------+-----------+-----+--------------+-----------+--------------+-------------+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+------+--------------------+------+----+-----+-----+----------+-------+-----+--------+-----------+-----+--------------+-----------+--------------+-------------+------------------+------------+\n",
      "|PassengerId|Pclass|                Name|   Sex| Age|SibSp|Parch|    Ticket|   Fare|Cabin|Embarked|Accompanied|Title|Pclass_indexed|Sex_indexed|Pclass_encoded|  Sex_encoded|       Age_imputed|Fare_imputed|\n",
      "+-----------+------+--------------------+------+----+-----+-----+----------+-------+-----+--------+-----------+-----+--------------+-----------+--------------+-------------+------------------+------------+\n",
      "|        902|     3|    Ilieff, Mr. Ylio|  male|null|    0|    0|    349220| 7.8958| null|       S|          0|  Mr.|           0.0|        0.0| (4,[0],[1.0])|(3,[0],[1.0])|26.497742339152797|      7.8958|\n",
      "|        914|     1|Flegenheim, Mrs. ...|female|null|    0|    0|  PC 17598|31.6833| null|       S|          0| Mrs.|           1.0|        1.0| (4,[1],[1.0])|(3,[1],[1.0])|35.660971473345526|     31.6833|\n",
      "|        921|     3|   Samaan, Mr. Elias|  male|null|    2|    0|      2662|21.6792| null|       C|          1|  Mr.|           0.0|        0.0| (4,[0],[1.0])|(3,[0],[1.0])|26.497742339152797|     21.6792|\n",
      "|        925|     3|\"Johnston, Mrs. A...|female|null|    1|    2|W./C. 6607|  23.45| null|       S|          1| Mrs.|           0.0|        1.0| (4,[0],[1.0])|(3,[1],[1.0])|21.982979570371622|       23.45|\n",
      "|        928|     3| Roth, Miss. Sarah A|female|null|    0|    0|    342712|   8.05| null|       S|          0|Miss.|           0.0|        1.0| (4,[0],[1.0])|(3,[1],[1.0])|21.982979570371622|        8.05|\n",
      "+-----------+------+--------------------+------+----+-----+-----+----------+-------+-----+--------+-----------+-----+--------------+-----------+--------------+-------------+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import regexp_extract, udf\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler, Imputer\n",
    "from pyspark.ml import Pipeline, Transformer, Estimator, Model\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "# Accompanied = binary feature = \"has either sibling, spouse, parent or child\"\n",
    "# stateless transformation => use a transformer\n",
    "class ExtractAccompaniedFeature(Transformer):\n",
    "    def transform(self, dataset, params=None):\n",
    "        return dataset.withColumn(\n",
    "            \"Accompanied\", \n",
    "            (dataset.SibSp + dataset.Parch >= 1).cast(IntegerType()) \n",
    "        )\n",
    "\n",
    "# Imput missing Embarked with most frequent value\n",
    "# stateful transformation => use an estimator\n",
    "class HandleMissingEmbarked(Estimator):\n",
    "    def fit(self, dataset, params=None):\n",
    "        mostFrequentValue = (dataset.groupby(\"Embarked\")\n",
    "                             .count()\n",
    "                             .orderBy(\"count\", ascending=False)\n",
    "                             .first()\n",
    "                             .Embarked\n",
    "                            )\n",
    "        return HandleMissingEmbarkedModel(mostFrequentValue)\n",
    "        \n",
    "class HandleMissingEmbarkedModel(Model):\n",
    "    \n",
    "    def __init__(self, mostFrequentValue):\n",
    "        self.mostFrequentValue = mostFrequentValue\n",
    "        \n",
    "    def transform(self, dataset, params=None):\n",
    "        return dataset.fillna(self.mostFrequentValue, \"Embarked\")\n",
    "\n",
    "# TItle regex processing\n",
    "@udf(returnType=StringType())\n",
    "def replace_title(s):\n",
    "    mrs_pattern = \"(Mme\\.|Ms\\.|Countess\\.|Lady\\.)\"\n",
    "    miss_pattern = \"(Mlle\\.)\"\n",
    "    mr_pattern = \"(Don\\.|Major\\.|Sir\\.|Col\\.|Capt\\.)\"\n",
    "    if re.search(mrs_pattern, s):\n",
    "        return re.sub(mrs_pattern, \"Mrs.\", s)\n",
    "    if re.search(miss_pattern, s):\n",
    "        return re.sub(miss_pattern, \"Miss.\", s)\n",
    "    if re.search(mr_pattern, s):\n",
    "        return re.sub(mr_pattern, \"Mr.\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "@udf\n",
    "def replace_empty(s):\n",
    "    if s == \"\":\n",
    "        return \"No-Title\"\n",
    "    return s\n",
    "\n",
    "# extraction of Title feature from Name\n",
    "class ExtractTitle(Transformer):\n",
    "    def transform(self, dataset, params=None):\n",
    "        titles_extract_pattern = r'(Mr\\.|Mrs\\.|Miss\\.|Master\\.|Dr\\.|Rev\\.)'\n",
    "        return ( dataset.withColumn(\"Title\", regexp_extract(\"Name\", titles_extract_pattern, 1))\n",
    "                .withColumn(\"Title\", replace_empty(\"Title\"))\n",
    "               )\n",
    "    \n",
    "# Imputing missing \"Age\" from regression of Pclass and Sex\n",
    "class HandleMissingAge(Estimator):\n",
    "    def __init__(self):\n",
    "        vect = VectorAssembler(\n",
    "            inputCols = [\"Pclass_encoded\", \"Sex_encoded\"], \n",
    "            outputCol='features_class_sex'\n",
    "        )\n",
    "        \n",
    "        lr = LinearRegression(\n",
    "            featuresCol=\"features_class_sex\",\n",
    "            labelCol='Age',\n",
    "            predictionCol='Age_imputed',\n",
    "            regParam = 0.3\n",
    "        )\n",
    "\n",
    "        self.pipe = Pipeline(\n",
    "            stages = [\n",
    "                vect,\n",
    "                lr\n",
    "            ])\n",
    "        self._params = None\n",
    "        self._paramMap = ParamGridBuilder().build()\n",
    "\n",
    "\n",
    "    def fit(self, dataset, params=None):\n",
    "        dataset_without_missing = dataset.where(col(\"Age\").isNotNull())\n",
    "        ageRegressor = self.pipe.fit(dataset_without_missing)\n",
    "        return HandleMissingAgeModel(ageRegressor)\n",
    "\n",
    "    \n",
    "class HandleMissingAgeModel(Model):\n",
    "    \n",
    "    def __init__(self, ageRegressor):\n",
    "        self.ageRegressor = ageRegressor\n",
    "        \n",
    "    def transform(self, dataset, params=None):\n",
    "        null_age_df = dataset.where(col(\"Age\").isNull())\n",
    "        not_null_age_df = dataset.where(col(\"Age\").isNotNull())\n",
    "              \n",
    "        null_age_df = (\n",
    "            self.ageRegressor\n",
    "            .transform(null_age_df)\n",
    "            .drop(\"features_class_sex\")\n",
    "            .cache()\n",
    "        )\n",
    "            \n",
    "        return null_age_df.union(\n",
    "            not_null_age_df.withColumn(\"Age_imputed\", col(\"Age\"))\n",
    "        )\n",
    "    \n",
    "extract_features = Pipeline(\n",
    "    stages = [\n",
    "        ExtractAccompaniedFeature(),\n",
    "        ExtractTitle(),\n",
    "        HandleMissingEmbarked(),\n",
    "        \n",
    "        # need to get Dummy categorisation of Pclass & Sex for Age Imputation\n",
    "        StringIndexer(inputCol = \"Pclass\", outputCol='Pclass_indexed', handleInvalid='keep'),\n",
    "        StringIndexer(inputCol = \"Sex\", outputCol='Sex_indexed', handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol = \"Pclass_indexed\", outputCol='Pclass_encoded', handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol = \"Sex_indexed\", outputCol='Sex_encoded', handleInvalid='keep'),\n",
    "        \n",
    "        HandleMissingAge(),\n",
    "        Imputer(inputCol = \"Fare\", outputCol='Fare_imputed')       \n",
    "])\n",
    "\n",
    "\n",
    "extract_features_fitted = extract_features.fit(train_df)\n",
    "train = extract_features_fitted.transform(train_df).cache()\n",
    "test = extract_features_fitted.transform(test_df).cache()\n",
    "\n",
    "train.show(5)\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of RandomForest, GBT and MLP classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlp] avg accuracy:\n",
      "0.797950914372483\n",
      "\n",
      "[rf] avg accuracy:\n",
      "0.8180353370304351\n",
      "\n",
      "[gbt] avg accuracy:\n",
      "0.8087038687528885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "preprocess = Pipeline(\n",
    "    stages = [\n",
    "        StringIndexer(inputCol = \"Embarked\", outputCol='Embarked_indexed', handleInvalid='keep'),\n",
    "        StringIndexer(inputCol = \"Title\", outputCol='Title_indexed', handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol = \"Embarked_indexed\", outputCol='Embarked_encoded', handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol = \"Accompanied\", outputCol='Accompanied_encoded', handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol = \"Title_indexed\", outputCol='Title_encoded', handleInvalid='keep'),\n",
    "        VectorAssembler(inputCols = [\"Fare_imputed\"], outputCol='Fare_vect'),\n",
    "        StandardScaler(withMean = True, inputCol = \"Fare_vect\", outputCol='Fare_std'),\n",
    "        VectorAssembler(inputCols = [\"Age_imputed\"], outputCol='Age_vect'),\n",
    "        StandardScaler(withMean = True, inputCol = \"Age_vect\", outputCol='Age_std'),        \n",
    "])\n",
    "\n",
    "inputCols = [\n",
    "    \"Pclass_encoded\", \n",
    "    \"Sex_encoded\", \n",
    "    \"Embarked_encoded\", \n",
    "    \"Accompanied_encoded\", \n",
    "    \"Title_encoded\",\n",
    "    \"Age_std\", \n",
    "    \"Fare_std\"]\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol = \"Survived\")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol = \"Survived\")\n",
    "\n",
    "va = VectorAssembler(\n",
    "    inputCols = inputCols,\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# here we have 23 inputs features corresponding to the 7 features in inputCols which are already encoded\n",
    "# can be checked with: \n",
    "# transformed = pipe_preprocessing.fit(train_df).transform(train_df)\n",
    "# va.transform(transformed).schema[\"features\"].metadata[\"ml_attr\"]\n",
    "layers = [23, 100, 2]\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol = \"Survived\",\n",
    "    layers=layers\n",
    ")\n",
    "\n",
    "trainedModels = dict()\n",
    "for classifier, classifier_name in zip( [mlp, rf, gbt], [\"mlp\", \"rf\", \"gbt\"]):\n",
    "    \n",
    "    clf = Pipeline(\n",
    "        stages = [\n",
    "        preprocess,\n",
    "        va,\n",
    "        classifier\n",
    "    ])\n",
    "\n",
    "    # Yes, accuracy is rather a poor evaluation metric\n",
    "    # but it is the metric defined in the Titanic kaggle competition\n",
    "    # the only way to get an accuracy evaluator seems to use MulticlassClassificationEvaluator\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", metricName=\"accuracy\")\n",
    "\n",
    "    # no paramter tuning for now, will perform it later on a gcp hadoop cluster    \n",
    "    paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "    # example of parameter tuning for GBT:\n",
    "    # paramGrid = ParamGridBuilder()\n",
    "    #   .addGrid(gbt.stepSize, [0.001, 0.03, 0.1, 0.3])\n",
    "    #   .addGrid(gbt.maxDepth, list(range(3,10))\n",
    "    #   .build()\n",
    "\n",
    "    cv = CrossValidator(\n",
    "        estimator=clf, \n",
    "        estimatorParamMaps=paramGrid, \n",
    "        evaluator=evaluator, \n",
    "        numFolds=3, \n",
    "        parallelism=2)\n",
    "\n",
    "    cv_model = cv.fit(train)\n",
    "    trainedModels[classifier_name] = cv_model.bestModel\n",
    "    print(f\"[{classifier_name}] avg accuracy:\\n{cv_model.avgMetrics[0]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Vote Ensemble Estimator (equivalent to sklearn.VotingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+-------------+--------------------+--------------+--------------------+--------+--------------------+\n",
      "|PassengerId|mlp_prediction|     mlp_probability|rf_prediction|      rf_probability|gbt_prediction|     gbt_probability|Survived|         probability|\n",
      "+-----------+--------------+--------------------+-------------+--------------------+--------------+--------------------+--------+--------------------+\n",
      "|        902|           0.0|[0.89811823244612...|          0.0|[0.87693451474159...|           0.0|[0.92569167436028...|       0|  0.9256916743602833|\n",
      "|        914|           1.0|[0.00134823822874...|          1.0|[0.08780913705043...|           1.0|[0.04802649602830...|       1|0.001348238228748...|\n",
      "|        921|           0.0|[0.88825387064665...|          0.0|[0.85532913983438...|           0.0|[0.89694843556073...|       0|  0.8969484355607384|\n",
      "|        925|           0.0|[0.60196394133403...|          1.0|[0.43975193736686...|           0.0|[0.65319669662432...|       0|  0.6531966966243233|\n",
      "|        928|           0.0|[0.60716923790313...|          1.0|[0.45166635265615...|           0.0|[0.78143600605456...|       0|  0.7814360060545672|\n",
      "|        931|           1.0|[0.32446550132040...|          0.0|[0.79475443072594...|           1.0|[0.49205300758274...|       1| 0.20524556927405438|\n",
      "|        933|           0.0|[0.55045153805900...|          0.0|[0.76273909807608...|           0.0|[0.54105226462919...|       0|  0.7627390980760816|\n",
      "|        939|           0.0|[0.90585323450031...|          0.0|[0.88026587170077...|           0.0|[0.87201797455075...|       0|  0.9058532345003103|\n",
      "|        946|           0.0|[0.88320351270935...|          0.0|[0.83273548431722...|           0.0|[0.61339907239456...|       0|  0.8832035127093504|\n",
      "|        950|           0.0|[0.94503417458628...|          0.0|[0.86824395597760...|           0.0|[0.9239040082921,...|       0|  0.9450341745862895|\n",
      "|        957|           1.0|[0.01545645944621...|          1.0|[0.14177998168862...|           1.0|[0.07807512482991...|       1|0.015456459446217149|\n",
      "|        968|           0.0|[0.89660056815689...|          0.0|[0.87014643527139...|           0.0|[0.90102275219567...|       0|  0.9010227521956752|\n",
      "|        975|           0.0|[0.89811823244612...|          0.0|[0.87693451474159...|           0.0|[0.92569167436028...|       0|  0.9256916743602833|\n",
      "|        976|           0.0|[0.87954985200929...|          0.0|[0.85507785005948...|           0.0|[0.56611413599900...|       0|  0.8795498520092906|\n",
      "|        977|           0.0|[0.91393481087028...|          0.0|[0.86139151043479...|           0.0|[0.89694843556073...|       0|  0.9139348108702812|\n",
      "|        980|           1.0|[0.34406779800088...|          1.0|[0.44986065358441...|           1.0|[0.18746590730646...|       1|  0.1874659073064601|\n",
      "|        983|           0.0|[0.89929443609416...|          0.0|[0.87693451474159...|           0.0|[0.87201797455075...|       0|  0.8992944360941698|\n",
      "|        985|           0.0|[0.89660056815689...|          0.0|[0.87014643527139...|           0.0|[0.90102275219567...|       0|  0.9010227521956752|\n",
      "|        994|           0.0|[0.90585323450031...|          0.0|[0.88026587170077...|           0.0|[0.87201797455075...|       0|  0.9058532345003103|\n",
      "|        999|           0.0|[0.90585323450031...|          0.0|[0.88026587170077...|           0.0|[0.87201797455075...|       0|  0.9058532345003103|\n",
      "+-----------+--------------+--------------------+-------------+--------------------+--------------+--------------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import array\n",
    "\n",
    "@udf(returnType = IntegerType())\n",
    "def majority_vote_prediction(predictions):\n",
    "    return int(np.mean(predictions) >= 0.5)\n",
    "\n",
    "@udf(returnType = DoubleType())\n",
    "def majority_vote_proba(predictions, probas):\n",
    "    prediction = int(np.mean(predictions) >= 0.5)\n",
    "    if prediction == 0:\n",
    "        # high probability for the class [Survived == 0]\n",
    "        return float(np.max(probas))\n",
    "    else:\n",
    "        return float(np.min(probas))\n",
    "\n",
    "class VotingClassifier(Model):\n",
    "    \n",
    "    def __init__(self, fittedModels):\n",
    "        self.fittedModels = fittedModels\n",
    "        \n",
    "    def transform(self, dataset, params=None):\n",
    "        all_predictions_df = None\n",
    "        for model_name, model in self.fittedModels.items():\n",
    "            prediction = (model.transform(dataset)\n",
    "                          .select(\"PassengerId\", \"prediction\", \"probability\")\n",
    "                          .withColumnRenamed(\"prediction\", f\"{model_name}_prediction\")\n",
    "                          .withColumnRenamed(\"probability\", f\"{model_name}_probability\")\n",
    "                         )\n",
    "            # merge all model predictions in a single dataframe\n",
    "            if not all_predictions_df:\n",
    "                all_predictions_df = prediction\n",
    "            else:\n",
    "                all_predictions_df = all_predictions_df.join(prediction, \"PassengerId\")\n",
    "        \n",
    "        \n",
    "        predictionCols = [col for col in all_predictions_df.columns if \"_prediction\" in col]\n",
    "        probabilityCols = [col for col in all_predictions_df.columns if \"_probability\" in col]\n",
    "        return (\n",
    "            all_predictions_df\n",
    "            .withColumn(\n",
    "                \"prediction\", \n",
    "                majority_vote_prediction(array([col(c) for c in predictionCols])))\n",
    "            .withColumn(\n",
    "                \"probability\", \n",
    "                majority_vote_proba(array([col(c) for c in predictionCols]), array([col(c) for c in probabilityCols])))\n",
    "            .withColumnRenamed(\"prediction\", \"Survived\")\n",
    "        )\n",
    "\n",
    "votingClf = VotingClassifier(trainedModels)\n",
    "final_prediction = votingClf.transform(test).cache()\n",
    "final_prediction.show()\n",
    "\n",
    "# store final prediction for kaggle submission\n",
    "(\n",
    "    final_prediction\n",
    "    .select([\"PassengerId\", \"Survived\"])\n",
    "    .coalesce(1)\n",
    "    .orderBy(\"PassengerId\")\n",
    "    .write.csv(\n",
    "        \"../submissions/titanic_ensemble_rf_gb_mlp_spark.csv\",\n",
    "        mode = \"overwrite\",\n",
    "        header = True\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## refacto\n",
    "- ajouter score kaggle\n",
    "- ajouter notebook sklearn Fare imput\n",
    "- ajouter commentaires explicatifs sur votingClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
