{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\n",
      "1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S\n",
      "2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C\n"
     ]
    }
   ],
   "source": [
    "!powershell Get-Content \"../data/train.csv\" -Head 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "spark = ( SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"titanic\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "train_df = spark.read.csv(\n",
    "    \"../data/train.csv\", \n",
    "    header = True,\n",
    "    inferSchema = True\n",
    ")\n",
    "\n",
    "train_df.printSchema()\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+------+------+-----+--------+-----------+-----+--------------+-----------+----------------+-------------+--------------+-------------+----------------+-------------------+-------------+------------+---------+--------------------+------------------+--------------------+--------------------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|Ticket|  Fare|Cabin|Embarked|Accompanied|Title|Pclass_indexed|Sex_indexed|Embarked_indexed|Title_indexed|Pclass_encoded|  Sex_encoded|Embarked_encoded|Accompanied_encoded|Title_encoded|Fare_imputed|Fare_vect|            Fare_std|       Age_imputed|            Age_vect|             Age_std|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+------+------+-----+--------+-----------+-----+--------------+-----------+----------------+-------------+--------------+-------------+----------------+-------------------+-------------+------------+---------+--------------------+------------------+--------------------+--------------------+\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|330877|8.4583| null|       Q|          0|  Mr.|           0.0|        0.0|             2.0|          0.0| (4,[0],[1.0])|(3,[0],[1.0])|   (4,[2],[1.0])|      (2,[0],[1.0])|(8,[0],[1.0])|      8.4583| [8.4583]|[-0.4778480503138...|26.497742339152797|[26.497742339152797]|[-0.2129111653727...|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|244373|  13.0| null|       S|          0|  Mr.|           2.0|        0.0|             0.0|          0.0| (4,[2],[1.0])|(3,[0],[1.0])|   (4,[0],[1.0])|      (2,[0],[1.0])|(8,[0],[1.0])|        13.0|   [13.0]|[-0.3864536722600...| 31.81582649258425| [31.81582649258425]|[0.18817248189229...|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|  2649| 7.225| null|       C|          0| Mrs.|           0.0|        1.0|             1.0|          2.0| (4,[0],[1.0])|(3,[1],[1.0])|   (4,[1],[1.0])|      (2,[0],[1.0])|(8,[2],[1.0])|       7.225|  [7.225]|[-0.5026662211428...|21.982979570371622|[21.982979570371622]|[-0.5534092593471...|\n",
      "|         27|       0|     3|Emir, Mr. Farred ...|  male|null|    0|    0|  2631| 7.225| null|       C|          0|  Mr.|           0.0|        0.0|             1.0|          0.0| (4,[0],[1.0])|(3,[0],[1.0])|   (4,[1],[1.0])|      (2,[0],[1.0])|(8,[0],[1.0])|       7.225|  [7.225]|[-0.5026662211428...|26.497742339152797|[26.497742339152797]|[-0.2129111653727...|\n",
      "|         29|       1|     3|\"O'Dwyer, Miss. E...|female|null|    0|    0|330959|7.8792| null|       Q|          0|Miss.|           0.0|        1.0|             2.0|          1.0| (4,[0],[1.0])|(3,[1],[1.0])|   (4,[2],[1.0])|      (2,[0],[1.0])|(8,[1],[1.0])|      7.8792| [7.8792]|[-0.4895015026182...|21.982979570371622|[21.982979570371622]|[-0.5534092593471...|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+------+------+-----+--------+-----------+-----+--------------+-----------+----------------+-------------+--------------+-------------+----------------+-------------------+-------------+------------+---------+--------------------+------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = false)\n",
      " |-- Accompanied: integer (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Pclass_indexed: double (nullable = false)\n",
      " |-- Sex_indexed: double (nullable = false)\n",
      " |-- Embarked_indexed: double (nullable = false)\n",
      " |-- Title_indexed: double (nullable = false)\n",
      " |-- Pclass_encoded: vector (nullable = true)\n",
      " |-- Sex_encoded: vector (nullable = true)\n",
      " |-- Embarked_encoded: vector (nullable = true)\n",
      " |-- Accompanied_encoded: vector (nullable = true)\n",
      " |-- Title_encoded: vector (nullable = true)\n",
      " |-- Fare_imputed: double (nullable = true)\n",
      " |-- Fare_vect: vector (nullable = true)\n",
      " |-- Fare_std: vector (nullable = true)\n",
      " |-- Age_imputed: double (nullable = true)\n",
      " |-- Age_vect: vector (nullable = true)\n",
      " |-- Age_std: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import regexp_extract, udf\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler, Imputer\n",
    "from pyspark.ml import Pipeline, Transformer, Estimator, Model\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "class ExtractAccompaniedFeature(Transformer):\n",
    "    def transform(self, dataset, params=None):\n",
    "        return dataset.withColumn(\n",
    "            \"Accompanied\", \n",
    "            (dataset.SibSp + dataset.Parch >= 1).cast(IntegerType()) \n",
    "        )\n",
    "\n",
    "class HandleMissingEmbarked(Estimator):\n",
    "    def fit(self, dataset, params=None):\n",
    "        mostFrequentValue = (dataset.groupby(\"Embarked\")\n",
    "                             .count()\n",
    "                             .orderBy(\"count\", ascending=False)\n",
    "                             .first()\n",
    "                             .Embarked\n",
    "                            )\n",
    "        return HandleMissingEmbarkedModel(mostFrequentValue)\n",
    "        \n",
    "class HandleMissingEmbarkedModel(Model):\n",
    "    \n",
    "    def __init__(self, mostFrequentValue):\n",
    "        self.mostFrequentValue = mostFrequentValue\n",
    "        \n",
    "    def transform(self, dataset, params=None):\n",
    "        return dataset.fillna(self.mostFrequentValue, \"Embarked\")\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def replace_title(s):\n",
    "    mrs_pattern = \"(Mme\\.|Ms\\.|Countess\\.|Lady\\.)\"\n",
    "    miss_pattern = \"(Mlle\\.)\"\n",
    "    mr_pattern = \"(Don\\.|Major\\.|Sir\\.|Col\\.|Capt\\.)\"\n",
    "    if re.search(mrs_pattern, s):\n",
    "        return re.sub(mrs_pattern, \"Mrs.\", s)\n",
    "    if re.search(miss_pattern, s):\n",
    "        return re.sub(miss_pattern, \"Miss.\", s)\n",
    "    if re.search(mr_pattern, s):\n",
    "        return re.sub(mr_pattern, \"Mr.\", s)\n",
    "    return s\n",
    "\n",
    "@udf\n",
    "def replace_empty(s):\n",
    "    if s == \"\":\n",
    "        return \"No-Title\"\n",
    "    return s\n",
    "\n",
    "class ExtractTitle(Transformer):\n",
    "    def transform(self, dataset, params=None):\n",
    "        titles_extract_pattern = r'(Mr\\.|Mrs\\.|Miss\\.|Master\\.|Dr\\.|Rev\\.)'\n",
    "        return ( dataset.withColumn(\"Title\", regexp_extract(\"Name\", titles_extract_pattern, 1))\n",
    "                .withColumn(\"Title\", replace_empty(\"Title\"))\n",
    "               )\n",
    "\n",
    "class HandleMissingAge(Estimator):\n",
    "    def __init__(self):\n",
    "        vect = VectorAssembler(\n",
    "            inputCols = [\"Pclass_encoded\", \"Sex_encoded\"], \n",
    "            outputCol='features_class_sex'\n",
    "        )\n",
    "        \n",
    "        lr = LinearRegression(\n",
    "            featuresCol=\"features_class_sex\",\n",
    "            labelCol='Age',\n",
    "            predictionCol='Age_imputed',\n",
    "            regParam = 0.3\n",
    "        )\n",
    "\n",
    "        self.pipe = Pipeline(\n",
    "            stages = [\n",
    "                vect,\n",
    "                lr\n",
    "            ])\n",
    "        self._params = None\n",
    "        self._paramMap = ParamGridBuilder().build()\n",
    "\n",
    "\n",
    "    def fit(self, dataset, params=None):\n",
    "        dataset_without_missing = dataset.where(col(\"Age\").isNotNull())\n",
    "        ageRegressor = self.pipe.fit(dataset_without_missing)\n",
    "        return HandleMissingAgeModel(ageRegressor)\n",
    "\n",
    "    \n",
    "class HandleMissingAgeModel(Model):\n",
    "    \n",
    "    def __init__(self, ageRegressor):\n",
    "        self.ageRegressor = ageRegressor\n",
    "        \n",
    "    def transform(self, dataset, params=None):\n",
    "        null_age_df = dataset.where(col(\"Age\").isNull())\n",
    "        not_null_age_df = dataset.where(col(\"Age\").isNotNull())\n",
    "              \n",
    "        null_age_df = (\n",
    "            self.ageRegressor\n",
    "            .transform(null_age_df)\n",
    "            .drop(\"features_class_sex\")\n",
    "            .cache()\n",
    "        )\n",
    "            \n",
    "        return null_age_df.union(\n",
    "            not_null_age_df.withColumn(\"Age_imputed\", col(\"Age\"))\n",
    "        )\n",
    "    \n",
    "pipe_preprocessing = Pipeline(\n",
    "    stages = [\n",
    "        ExtractAccompaniedFeature(),\n",
    "        ExtractTitle(),\n",
    "        HandleMissingEmbarked(),\n",
    "        StringIndexer(inputCol = \"Pclass\", outputCol='Pclass_indexed', handleInvalid='keep'),\n",
    "        StringIndexer(inputCol = \"Sex\", outputCol='Sex_indexed', handleInvalid='keep'),\n",
    "        StringIndexer(inputCol = \"Embarked\", outputCol='Embarked_indexed', handleInvalid='keep'),\n",
    "        StringIndexer(inputCol = \"Title\", outputCol='Title_indexed', handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol = \"Pclass_indexed\", outputCol='Pclass_encoded', handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol = \"Sex_indexed\", outputCol='Sex_encoded', handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol = \"Embarked_indexed\", outputCol='Embarked_encoded', handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol = \"Accompanied\", outputCol='Accompanied_encoded', handleInvalid='keep'),\n",
    "        OneHotEncoder(inputCol = \"Title_indexed\", outputCol='Title_encoded', handleInvalid='keep'),\n",
    "        Imputer(inputCol = \"Fare\", outputCol='Fare_imputed'),\n",
    "        VectorAssembler(inputCols = [\"Fare_imputed\"], outputCol='Fare_vect'),\n",
    "        StandardScaler(withMean = True, inputCol = \"Fare_vect\", outputCol='Fare_std'),\n",
    "        HandleMissingAge(),\n",
    "        VectorAssembler(inputCols = [\"Age_imputed\"], outputCol='Age_vect'),\n",
    "        StandardScaler(withMean = True, inputCol = \"Age_vect\", outputCol='Age_std'),        \n",
    "])\n",
    "\n",
    "transformed = pipe_preprocessing.fit(train_df).transform(train_df)\n",
    "transformed.show(5)\n",
    "transformed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of RandomForest, GBT and MLP classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlp] avg accuracy:\n",
      "0.8135417783017184\n",
      "\n",
      "[rf] avg accuracy:\n",
      "0.8329743150073232\n",
      "\n",
      "[gbt] avg accuracy:\n",
      "0.8246222269853178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "inputCols = [\n",
    "    \"Pclass_encoded\", \n",
    "    \"Sex_encoded\", \n",
    "    \"Embarked_encoded\", \n",
    "    \"Accompanied_encoded\", \n",
    "    \"Title_encoded\",\n",
    "    \"Age_std\", \n",
    "    \"Fare_std\"]\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol = \"Survived\")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol = \"Survived\")\n",
    "\n",
    "va = VectorAssembler(\n",
    "    inputCols = inputCols,\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "# here we have 23 inputs features corresponding to the 7 features in inputCols which are already encoded\n",
    "# can be checked with: \n",
    "# transformed = pipe_preprocessing.fit(train_df).transform(train_df)\n",
    "# va.transform(transformed).schema[\"features\"].metadata[\"ml_attr\"]\n",
    "layers = [23, 100, 2]\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol = \"Survived\",\n",
    "    layers=layers\n",
    ")\n",
    "\n",
    "trainedModels = dict()\n",
    "for classifier, classifier_name in zip( [mlp, rf, gbt], [\"mlp\", \"rf\", \"gbt\"]):\n",
    "    \n",
    "    clf = Pipeline(\n",
    "        stages = [\n",
    "        pipe_preprocessing,\n",
    "        va,\n",
    "        classifier\n",
    "    ])\n",
    "\n",
    "    # Yes, accuracy is rather a poor evaluation metric\n",
    "    # but it is the metric defined in the Titanic kaggle competition\n",
    "    # the only way to get an accuracy evaluator seems to use MulticlassClassificationEvaluator\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", metricName=\"accuracy\")\n",
    "\n",
    "    # no paramter tuning for now, will perform it later on a gcp hadoop cluster    \n",
    "    paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "    # example of parameter tuning for GBT:\n",
    "    # paramGrid = ParamGridBuilder()\n",
    "    #   .addGrid(gbt.stepSize, [0.001, 0.03, 0.1, 0.3])\n",
    "    #   .addGrid(gbt.maxDepth, list(range(3,10))\n",
    "    #   .build()\n",
    "\n",
    "    cv = CrossValidator(\n",
    "        estimator=clf, \n",
    "        estimatorParamMaps=paramGrid, \n",
    "        evaluator=evaluator, \n",
    "        numFolds=3, \n",
    "        parallelism=2)\n",
    "\n",
    "    cv_model = cv.fit(train_df)\n",
    "    trainedModels[classifier_name] = cv_model.bestModel\n",
    "    print(f\"[{classifier_name}] avg accuracy:\\n{cv_model.avgMetrics[0]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = spark.read.csv(\n",
    "    \"../data/test.csv\", \n",
    "    header = True,\n",
    "    inferSchema = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+-------------+--------------------+--------------+--------------------+--------+--------------------+\n",
      "|PassengerId|mlp_prediction|     mlp_probability|rf_prediction|      rf_probability|gbt_prediction|     gbt_probability|Survived|         probability|\n",
      "+-----------+--------------+--------------------+-------------+--------------------+--------------+--------------------+--------+--------------------+\n",
      "|        902|           0.0|[0.90258528937260...|          0.0|[0.87753908566204...|           0.0|[0.92569167436028...|       0|  0.9256916743602833|\n",
      "|        914|           1.0|[0.01919100393759...|          1.0|[0.09467533240999...|           1.0|[0.04802649602830...|       1|0.019191003937591558|\n",
      "|        921|           0.0|[0.79052197972662...|          0.0|[0.83356564208925...|           0.0|[0.89694843556073...|       0|  0.8969484355607384|\n",
      "|        925|           0.0|[0.62835657223186...|          0.0|[0.53296327576042...|           0.0|[0.65319669662432...|       0|  0.6531966966243233|\n",
      "|        928|           0.0|[0.51779792434453...|          1.0|[0.45906632886063...|           0.0|[0.78143600605456...|       0|  0.7814360060545672|\n",
      "|        931|           1.0|[0.34769755103959...|          0.0|[0.84378109625784...|           1.0|[0.49205300758274...|       1| 0.15621890374215064|\n",
      "|        933|           0.0|[0.56059324738090...|          0.0|[0.75891907024245...|           0.0|[0.54105226462919...|       0|  0.7589190702424509|\n",
      "|        939|           0.0|[0.91596713268783...|          0.0|[0.86466528638810...|           0.0|[0.87201797455075...|       0|  0.9159671326878303|\n",
      "|        946|           0.0|[0.86853437704473...|          0.0|[0.83963972790348...|           0.0|[0.61339907239456...|       0|  0.8685343770447366|\n",
      "|        950|           0.0|[0.93525277974237...|          0.0|[0.85846424507896...|           0.0|[0.9239040082921,...|       0|  0.9352527797423741|\n",
      "|        957|           1.0|[0.08151737300318...|          1.0|[0.14492972890821...|           1.0|[0.07807512482991...|       1| 0.07807512482991329|\n",
      "|        968|           0.0|[0.90123902843120...|          0.0|[0.87120398982595...|           0.0|[0.90102275219567...|       0|  0.9012390284312088|\n",
      "|        975|           0.0|[0.90258528937260...|          0.0|[0.87753908566204...|           0.0|[0.92569167436028...|       0|  0.9256916743602833|\n",
      "|        976|           0.0|[0.96232807663232...|          0.0|[0.81499512581354...|           0.0|[0.56611413599900...|       0|  0.9623280766323287|\n",
      "|        977|           0.0|[0.77560872910694...|          0.0|[0.86507393826067...|           0.0|[0.89694843556073...|       0|  0.8969484355607384|\n",
      "|        980|           1.0|[0.09920164670451...|          1.0|[0.41206083436263...|           1.0|[0.18746590730646...|       1| 0.09920164670451884|\n",
      "|        983|           0.0|[0.90362903034964...|          0.0|[0.87753908566204...|           0.0|[0.87201797455075...|       0|   0.903629030349642|\n",
      "|        985|           0.0|[0.90123902843120...|          0.0|[0.87120398982595...|           0.0|[0.90102275219567...|       0|  0.9012390284312088|\n",
      "|        994|           0.0|[0.91596713268783...|          0.0|[0.86466528638810...|           0.0|[0.87201797455075...|       0|  0.9159671326878303|\n",
      "|        999|           0.0|[0.91596713268783...|          0.0|[0.86466528638810...|           0.0|[0.87201797455075...|       0|  0.9159671326878303|\n",
      "+-----------+--------------+--------------------+-------------+--------------------+--------------+--------------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import array\n",
    "\n",
    "@udf(returnType = IntegerType())\n",
    "def majority_vote_prediction(predictions):\n",
    "    return int(np.mean(predictions) >= 0.5)\n",
    "\n",
    "@udf(returnType = DoubleType())\n",
    "def majority_vote_proba(predictions, probas):\n",
    "    prediction = int(np.mean(predictions) >= 0.5)\n",
    "    if prediction == 0:\n",
    "        # high probability for the class [Survived == 0]\n",
    "        return float(np.max(probas))\n",
    "    else:\n",
    "        return float(np.min(probas))\n",
    "\n",
    "class VotingClassifier(Model):\n",
    "    \n",
    "    def __init__(self, fittedModels):\n",
    "        self.fittedModels = fittedModels\n",
    "        \n",
    "    def transform(self, dataset, params=None):\n",
    "        all_predictions_df = None\n",
    "        for model_name, model in self.fittedModels.items():\n",
    "            prediction = (model.transform(dataset)\n",
    "                          .select(\"PassengerId\", \"prediction\", \"probability\")\n",
    "                          .withColumnRenamed(\"prediction\", f\"{model_name}_prediction\")\n",
    "                          .withColumnRenamed(\"probability\", f\"{model_name}_probability\")\n",
    "                          .cache()\n",
    "                         )\n",
    "            # merge all model predictions in a single dataframe\n",
    "            if not all_predictions_df:\n",
    "                all_predictions_df = prediction\n",
    "            else:\n",
    "                all_predictions_df = all_predictions_df.join(prediction, \"PassengerId\")\n",
    "        \n",
    "        \n",
    "        predictionCols = [col for col in all_predictions_df.columns if \"_prediction\" in col]\n",
    "        probabilityCols = [col for col in all_predictions_df.columns if \"_probability\" in col]\n",
    "        return (\n",
    "            all_predictions_df\n",
    "            .withColumn(\n",
    "                \"prediction\", \n",
    "                majority_vote_prediction(array([col(c) for c in predictionCols])))\n",
    "            .withColumn(\n",
    "                \"probability\", \n",
    "                majority_vote_proba(array([col(c) for c in predictionCols]), array([col(c) for c in probabilityCols])))\n",
    "            .withColumnRenamed(\"prediction\", \"Survived\")\n",
    "        )\n",
    "\n",
    "votingClassifier = VotingClassifier(trainedModels)\n",
    "final_prediction = votingClassifier.transform(test_df).cache()\n",
    "final_prediction.show()\n",
    "final_prediction\n",
    "(\n",
    "    final_prediction\n",
    "    .select([\"PassengerId\", \"Survived\"])\n",
    "    .coalesce(1)\n",
    "    .orderBy(\"PassengerId\")\n",
    "    .write.csv(\n",
    "        \"../submissions/titanic_ensemble_rf_gb_mlp_spark.csv\",\n",
    "        mode = \"overwrite\",\n",
    "        header = True\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## refacto\n",
    "- séparer feature extraction from model training\n",
    "- ajouter score kaggle\n",
    "- ajouter notebook sklearn Fare imput\n",
    "- caching optimization\n",
    "- ajouter intro, focus sur les transformer, estimator, models customs\n",
    "- ajouter la fonction de détection des nulls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
